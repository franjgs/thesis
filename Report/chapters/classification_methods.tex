\chapter{Classification Methods}
\label{chapter:Classification Methods}

Machine learning is a branch of computer science that deals with building and analyzing systems that are able to learn from data. Algorithms based upon such analysis involve constructing a model from a given dataset, and then using this model to perform required tasks. Machine learning techniques can broadly be divided into two categories - supervised learning, and unsupervised learning.
\begin{itemize}
    \item{
    Supervised Learning\\
    Methods falling in this category operate in two phases
    \begin{itemize}
        \item{In the first step, we assume the availability of training data, which is used to build a model so that it takes into account the structure of the given dataset}
        \item{In the second step, we use this model to make predictions on the testing data (the real world data). This is the data that the model has not seen yet, and is required to make predictions on.}
    \end{itemize}
    }
    \item{
    Unsupervised Learning\\
    Methods falling under this category operate in a single phase. It starts with a model with zero knowledge about the structure of the given dataset. As we feed data into the model, it continuously learns the structure of the given dataset and calculates the predictions based on this knowledge. The main difference between this family of algorithms and supervised learning algorithms is the presence/absence of training labels.
    }
\end{itemize}
The primary focus in this thesis remains on supervised learning methods.

\section{Introduction}
Classification is one of the fundamental problems in machine learning. Given a dataset $\mathbf{X}$, it is required to separate the samples contained within the dataset into two (or more than two, depending on the input) classes. Formally, given a dataset that contains $N$ instances $(\mathbf{X}_{n}, \mathbf{Y}_n) |_{n = 1}^{N}$, where each instance ($\mathbf{X}_{n}, \mathbf{y}_{n}$) is of the form $[(\mathbf{x}_{n, 1}, \mathbf{x}_{n, 2}, ... \mathbf{x}_{n, D}), \mathbf{y}_{n}]$, where each $\mathbf{x}_{n}$ is the value of the feature $k \in [1, D]$, and $\mathbf{y}_{n}$ is the label of the sample which can take a limited number of possible values, the aim is to calculate the value of $\mathbf{y}_{n}$, given the feature information. This separation can usually be done using a supervised learning method, in which case we are given the training data (on which the model is built) and are required to predict the labels of the test data, or using unsupervised methods, where the model is required to identify the categories of the samples without any information on $\mathbf{y}$.

The performance of a particular classifier varies with the type of the data to be classified. Not all classifiers are good for all classes of problems. Some classifiers suit a particular problem more than some others; choosing a classifier for a problem still remains a decision which may or may not be completely scientific, even though there have been a number of tests been done to correlate classifier performance with data type.

\section{Support Vector Machines}
Support Vector Machines (SVMs) form a fairly popular class of machine learning algorithms used mainly for binary classification and regression analysis. Given the training data, the goal of an SVM is to find a decision boundary (a hyperplane in a high or infinite dimensional space) that separates the two classes of data while maximizing the distance of the boundary from any data point. The resulting decision function is fully specified by a (usually small) subset of the data, and the points in this subset are referred to as support vectors.

All classifiers resort to a distance function, in some form or the other, that can provide a similarity measurement between two points. In the simplest form of an SVM, the distance function is simply the dot product between two points, and such SVMs are referred to as \emph{Linear Support Vector Machines}. In the case that a simple linear SVM is not able to find a sufficiently accurate decision boundary that can separate the data points into two classes (simply because the input data is not linearly separable), we use the \emph{kernel trick}, transforming the data from a low dimensional space to a much higher dimensional feature space using an appropriate function $\phi(\mathbf{x}): \mathbf{X} \in \mathbb{R}^{L} \rightarrow \mathbb{R}^H (L \ll H)$, and then using the kernel function $\mathbf{K(\phi(x), \phi(y))}$, which makes the separation easier. The trick involved is that even though the transformation $\phi(x)$ may be expensive, computing the final similarity value $\mathbf{K(\phi(x), \phi(y))}$ is not.

The most popular kernel functions include
\begin{itemize}
    \item{Linear (the simple SVM) - $\mathbf{K(x, y)} = (\mathbf{x} \cdot \mathbf{y})$ }
    \item{Polynomial - $\mathbf{K(x, y)} = (\gamma \mathbf{x} \cdot \mathbf{y} + \mathbf{c})^{\mathbf{d}}$}
    \item{Radial Basis - $\mathbf{K(x, y)} = \exp(-\gamma {| \mathbf{x} - \mathbf{y} |}^{2})$}
    \item{Sigmoid - $\mathbf{K(x, y)} = \tanh(\gamma \mathbf{x} \cdot \mathbf{y} + \mathbf{c})$}
\end{itemize}

\section{Multiple Kernel Learning}
Multiple Kernel Learning makes an improvement to standalone Support Vector Machines by using multiple kernels instead of a single one. In contrast to standard SVMs which use a single kernel function to calculate the similarity score between two data points, MKL algorithms combine scores from multiple kernels to obtain one single score, which is then used as the final similarity score. As summarized by \cite{gonen11a}, multiple kernel learning algorithms can be put into 12 major categories, based on some of their key properties (learning method, functional form, target function, training method, base learner, and the computational complexity).

The two main uses of MKL algorithms are also discussed by \cite{gonen11a}. The first one uses the fact that since different kernels may be used as different notions of similarity, we may maximize the accuracy obtained from each kernel, and then let the learning algorithm decide whether one kernel (which works better than the rest) or a combination of kernels is suitable for the task at hand. The second use is a more traditional use, where different kernels corresponding to different notions of similarity are combined, either in a linear or in a non-linear fashion to obtain a single kernel. As with all machine learning algorithms, multiple kernel learning does not suit all possible use cases, but in a lot of cases, combining multiple kernels may result in increased accuracy.
